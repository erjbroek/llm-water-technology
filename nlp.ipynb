{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "%pip install pypdf\n",
    "%pip install python-docx\n",
    "from docx import Document\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf14350",
   "metadata": {},
   "source": [
    "## Loading of the files\n",
    "Loading the files the same way as done during the data understanding. We make a Path object, which we then can use to loop through every file in the \"Trial\" directory. This directory holds 2 files used during testing, to mainly check and test the natural language processor out. Next, basic meta deta for each of these files is stored inside of a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db566ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "path = Path('Trial')\n",
    "\n",
    "for file in path.rglob(\"*\"):\n",
    "  if file.is_file():\n",
    "    files.append({\n",
    "      \"path\": str(file),\n",
    "      \"project name\": file.relative_to(path).parts[0],\n",
    "      \"extension\": file.suffix.lower(),\n",
    "      \"size_bytes\": file.stat().st_size,\n",
    "      \"size_mb\": file.stat().st_size / (1024 * 1024),\n",
    "      \"folder_depth\": len(file.relative_to(path).parents)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98562189",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.DataFrame(files)\n",
    "files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1447f",
   "metadata": {},
   "source": [
    "## Looping through the dataframe using pydf\n",
    "Looping through the dataframe through each row.\n",
    "Creating a reader object using pypdf pdfreader.\n",
    "Looping through each page in the readerobject and counting the lenght of the page\n",
    "Counting the words by splitting the text of each page in the readerobject and summing it.\n",
    "Extracting the text using extract_text from pydf.\n",
    "\n",
    "First we start by looping through every file in the given directory/dataframe. For each file, it first checks what the file type is, after which several points of information are gathered:\n",
    "1. Checks language: based on most occuring words in both english and dutich vocabularies, the language is selected depending on which words occurs most\n",
    "2. Number of pages: we can simply look at the amount of values in the pages property of the reader object.\n",
    "3. Word count: for each page, the total amount of words is added to the total amount of words\n",
    "4. Text: here, the actual content of the documents is stored\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_file_df = files_df.copy()\n",
    "pdf_files = []\n",
    "\n",
    "# maybe not the best way to determine language, but it seems to work perfectly\n",
    "most_occuring_english_words = [\"the\", \"be\", \"to\", \"and\"]\n",
    "most_occuring_dutch_words = [\"de\", \"het\", \"of\", \"een\"]\n",
    "\n",
    "complete_file_df['num_pages'] = None\n",
    "complete_file_df['word_count'] = None\n",
    "for i, document in complete_file_df.iterrows():\n",
    "  \n",
    "  if document['extension'] == '.pdf':\n",
    "    reader_object = pypdf.PdfReader(document['path'])\n",
    "    language_text = \"\"\n",
    "    for page in reader_object.pages:\n",
    "      if page_text := page.extract_text():\n",
    "        language_text += page_text\n",
    "    \n",
    "    english_score = 0\n",
    "    dutch_score = 0\n",
    "    for word in language_text.split():\n",
    "      if word.lower() in most_occuring_english_words:\n",
    "        english_score += 1\n",
    "      elif word.lower() in most_occuring_dutch_words:\n",
    "        dutch_score += 1\n",
    "    \n",
    "    complete_file_df.at[i, 'language'] = 'english' if english_score > dutch_score else 'dutch' if dutch_score > english_score else 'unknown'\n",
    "\n",
    "\n",
    "    # since if you use ['column_name'].iloc[index], you get FutureWarning: ChainedAssignmentError\n",
    "    # its the reason why i use .at[index, 'column_name'] instead\n",
    "    complete_file_df.at[i, 'num_pages'] = len(reader_object.pages)\n",
    "    complete_file_df.at[i, 'word_count'] = sum(len(text.split()) for page in reader_object.pages if (text := page.extract_text()))\n",
    "    complete_file_df.at[i, 'text'] = language_text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used as validation to check if the amount of words are even somewhat close to the files.\n",
    "# for the 2 test files used, it was quite accurate (first file 10 words off, the second 500-ish)\n",
    "\n",
    "for i,row in complete_file_df.iterrows():\n",
    "    print(len(complete_file_df.at[i, 'text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cdab1",
   "metadata": {},
   "source": [
    "# preparing the text\n",
    "After the text has been succesfully loaded, we can start actually preparing the text. For this, the following things were done:\n",
    "\n",
    "- translation (found another currently better alternative): we initialy used GoogleTranslator to translate the text. There were however 2 issues with this: 1. 5000 max character limit | 2. very slow, taking atleast 30 min for the 2 documents. The code for this is currently commented out.\n",
    "- tokenizing: instead of using the raw text as imput, we first tokanize it. While tokens are similar to words, they are not the same.\n",
    "- chunking: based on the chunksize, the text is split into several chunks (for example, chunksize of 500 would return pieces of text 500 tokens long)\n",
    "\n",
    "Regarding the warning, as far as we understand this shouldn't be a big issue:\n",
    "since we tokanize the text first before chunking, it gives the warning to make sure the chunksize shouldn't be larger than 500 since the limit is something like 512? However, because we chunk after tokanising, we split the tokens anyways and we don't have a single chunk of 43192 tokens (which (as far as we know) the warning is about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98456ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "chunks = []\n",
    "chunksize = 500\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "\n",
    "for i, document in complete_file_df.iterrows():\n",
    "    print(f\"document {i+1} of {len(complete_file_df)}\")\n",
    "    text = document['text']\n",
    "\n",
    "    # if document['language'] == 'dutch':\n",
    "    #     words = text.split()\n",
    "    #     sentences = []\n",
    "    #     current = \"\"\n",
    "    #     max_characters = 5000\n",
    "\n",
    "    #     for word in words:\n",
    "    #         if len(current) + len(word) + (1 if current else 0) <= max_characters:\n",
    "    #             current += (\" \" if current else \"\") + word\n",
    "    #         else:\n",
    "    #             sentences.append(current)\n",
    "    #             current = word\n",
    "    #         if current:\n",
    "    #             sentences.append(current)\n",
    "    \n",
    "    #     print(f\"  split into {len(sentences)} sentences\")\n",
    "    #     text = \"\"\n",
    "    #     for sentence in sentences:\n",
    "    #         translate_text = GoogleTranslator(source='auto', target='en').translate(sentence)\n",
    "    #         text += translate_text + \" \"\n",
    "\n",
    "    #     print(f\"  translated to {document['language']}\")\n",
    "\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"tokenized into {len(tokens)} tokens\")\n",
    "    \n",
    "    for j in range(0, len(tokens),chunksize):\n",
    "        chunk = tokens[j:j+chunksize]\n",
    "        chunk = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk)\n",
    "        # print(chunk)\n",
    "    print(\" \")\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/xlm-roberta-base-squad2\",tokenizer=tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3b4b2",
   "metadata": {},
   "source": [
    "# defining the model\n",
    "Here, we implement and use the question & answer pipeline from `from transformers import pipeline`. For this, we first defined our nlp model. For this, `deepset/xlm-roberta-base-squad2` seemed to handle the different languages. \n",
    "We first use lower to change everything from the given question into lowercase. next:\n",
    "1. loop throug every single chunk\n",
    "2. simply check if the chunk has matching key words\n",
    "3. if any keywords match, they go through the qa_pipeline mentioned above.\n",
    "4. this pipeline returns the expected answer with the highest confidence, along with the chunk the answer was in.\n",
    "5. At last, the top 3 answers with the highest confidence is stored inside `best_answer`\n",
    "\n",
    "example:\n",
    "- question: `What is the budget of water nexus`\n",
    "- response: `{'score': 1.6871259659528732, 'start': 1138, 'end': 1154, 'answer': '6.6 Million Euro', 'chunk': 1}`\n",
    "\n",
    "which is so close to correct, but the model missed the additional 1.2 something million mentioned a few sentences later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52deaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_answer = [\n",
    "    {\"score\": 0.0, \"answer\": \"No relevant information found.\", \"chunk\": 0},\n",
    "    {\"score\": 0.0, \"answer\": \"No relevant information found.\", \"chunk\": 0},\n",
    "    {\"score\": 0.0, \"answer\": \"No relevant information found.\", \"chunk\": 0}\n",
    "]\n",
    "\n",
    "def document_qa(question, text_chunks, qa_model_pipeline):\n",
    "    question_words = set(question.lower().split())\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        # Simple check: if a chunk contains any key words, process it\n",
    "        if any(word in chunk.lower() for word in question_words):\n",
    "            try:\n",
    "                result = qa_model_pipeline(question=question, context=chunk)\n",
    "                \n",
    "                # If this chunk's answer is better than the current best answer, save it.\n",
    "                if result['score'] > best_answer[0][\"score\"]:\n",
    "                    best_answer[0] = result\n",
    "                    best_answer[0]['chunk'] = i\n",
    "                elif result['score'] > best_answer[1][\"score\"]:\n",
    "                    best_answer[1] = result\n",
    "                    best_answer[1]['chunk'] = i\n",
    "                elif result['score'] > best_answer[2][\"score\"]:\n",
    "                    best_answer[2] = result\n",
    "                    best_answer[2]['chunk'] = i\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {i}: {e}\")\n",
    "                continue\n",
    "                 \n",
    "# Example Question:\n",
    "question = \"What is the budget of water nexus\"\n",
    "# question = \"What is the yield of growing vegetables in open ground?\"\n",
    "# question=\"Wat is een boon\"\n",
    "\n",
    "result = document_qa(question, chunks, qa_pipeline)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Question: {question}\")\n",
    "for answer in best_answer:\n",
    "    print(f\"Response: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
